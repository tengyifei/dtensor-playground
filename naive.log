W0615 22:46:34.257000 19592 torch/distributed/run.py:793] 
W0615 22:46:34.257000 19592 torch/distributed/run.py:793] *****************************************
W0615 22:46:34.257000 19592 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 22:46:34.257000 19592 torch/distributed/run.py:793] *****************************************
============================================================
Initial tensor placements:
Input: (Shard(dim=0), Shard(dim=1)) on shape torch.Size([1024, 2048])
W_in: (Shard(dim=0), Shard(dim=1)) on shape torch.Size([2048, 16384])
W_out: (Shard(dim=1), Shard(dim=0)) on shape torch.Size([16384, 2048])
============================================================

FORWARD PASS
============================================================

Step 1: In @ W_in
Before matmul:
  Input: (Shard(dim=0), Shard(dim=1))
  W_in: (Shard(dim=0), Shard(dim=1))
[rank6]:V0615 22:46:40.254000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank5]:V0615 22:46:40.254000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank2]:V0615 22:46:40.254000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank4]:V0615 22:46:40.254000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank0]:V0615 22:46:40.254000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank1]:V0615 22:46:40.254000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank7]:V0615 22:46:40.254000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank3]:V0615 22:46:40.254000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Shard(dim=0), Shard(dim=1)) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384)) @ mesh: (4, 2))
[rank1]:V0615 22:46:40.260000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank0]:V0615 22:46:40.260000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank5]:V0615 22:46:40.260000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank2]:V0615 22:46:40.260000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank6]:V0615 22:46:40.260000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank4]:V0615 22:46:40.260000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank1]:V0615 22:46:40.260000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank0]:V0615 22:46:40.260000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank2]:V0615 22:46:40.260000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank6]:V0615 22:46:40.260000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank5]:V0615 22:46:40.260000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank4]:V0615 22:46:40.260000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank3]:V0615 22:46:40.262000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank7]:V0615 22:46:40.262000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Partial(sum), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.mm.default, args_schema=(Spec((Shard(dim=1), Replicate()) on (1024, 2048)), Spec((Shard(dim=0), Shard(dim=1)) on (2048, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank7]:V0615 22:46:40.263000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank3]:V0615 22:46:40.263000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to R on mesh dim 1
[rank4]:V0615 22:46:40.270000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank1]:V0615 22:46:40.270000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank5]:V0615 22:46:40.270000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank0]:V0615 22:46:40.270000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank2]:V0615 22:46:40.271000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank6]:V0615 22:46:40.271000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank3]:V0615 22:46:40.284000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank7]:V0615 22:46:40.289000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(0) to S(1) on mesh dim 0
[rank1]:V0615 22:46:40.509000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank1]:V0615 22:46:40.512000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank1]:V0615 22:46:40.513000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank5]:V0615 22:46:40.518000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank5]:V0615 22:46:40.521000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank5]:V0615 22:46:40.521000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank3]:V0615 22:46:40.527000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank7]:V0615 22:46:40.529000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank3]:V0615 22:46:40.532000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank3]:V0615 22:46:40.533000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0

Intermediate result: (Partial(sum), Shard(dim=1)) on shape torch.Size([1024, 16384])
[rank4]:V0615 22:46:40.533000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank7]:V0615 22:46:40.533000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank0]:V0615 22:46:40.533000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank7]:V0615 22:46:40.534000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank6]:V0615 22:46:40.534000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank2]:V0615 22:46:40.535000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.gelu.default, args_schema=Spec((Partial(sum), Shard(dim=1)) on (1024, 16384)) @ mesh: (4, 2))
[rank4]:V0615 22:46:40.536000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank4]:V0615 22:46:40.537000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank0]:V0615 22:46:40.537000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank0]:V0615 22:46:40.537000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank6]:V0615 22:46:40.538000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank6]:V0615 22:46:40.538000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank2]:V0615 22:46:40.538000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.gelu.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Replicate(), Shard(dim=1)), tensor_meta=TensorMeta(shape=torch.Size([1024, 16384]), stride=(16384, 1), dtype=torch.float32)), redistribute_schema=OpSchema(op=aten.gelu.default, args_schema=(Spec((Replicate(), Shard(dim=1)) on (1024, 16384))), kwargs_schema={}), needs_redistribute=True)
[rank2]:V0615 22:46:40.538000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 0
[rank3]:V0615 22:46:40.547000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank5]:V0615 22:46:40.548000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank7]:V0615 22:46:40.548000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank1]:V0615 22:46:40.548000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank3]:V0615 22:46:40.550000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank3]:V0615 22:46:40.550000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank5]:V0615 22:46:40.550000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank7]:V0615 22:46:40.551000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank3]:V0615 22:46:40.551000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank1]:V0615 22:46:40.551000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank5]:V0615 22:46:40.551000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank7]:V0615 22:46:40.551000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank6]:V0615 22:46:40.552000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank1]:V0615 22:46:40.552000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank3]:V0615 22:46:40.552000 19629 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank5]:V0615 22:46:40.552000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank7]:V0615 22:46:40.552000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank4]:V0615 22:46:40.552000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))

Step 2: Intermediate @ W_out
Before matmul:
  Intermediate: (Replicate(), Shard(dim=1))
  W_out: (Shard(dim=1), Shard(dim=0))
[rank1]:V0615 22:46:40.552000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank7]:V0615 22:46:40.553000 19633 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank5]:V0615 22:46:40.553000 19631 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank2]:V0615 22:46:40.553000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank0]:V0615 22:46:40.553000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:167] Dispatching op_call: Op(op=aten.mm.default, args_schema=Spec((Replicate(), Shard(dim=1)) on (1024, 16384)), Spec((Shard(dim=1), Shard(dim=0)) on (16384, 2048)) @ mesh: (4, 2))
[rank1]:V0615 22:46:40.553000 19627 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank6]:V0615 22:46:40.554000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank6]:V0615 22:46:40.555000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank4]:V0615 22:46:40.555000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank6]:V0615 22:46:40.555000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank2]:V0615 22:46:40.555000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)
[rank4]:V0615 22:46:40.555000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank0]:V0615 22:46:40.555000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py:171] output_sharding for aten.mm.default: OutputSharding(output_spec=DTensorSpec(mesh=DeviceMesh('cuda', [[0, 1], [2, 3], [4, 5], [6, 7]], mesh_dim_names=('fsdp', 'tp')), placements=(Shard(dim=1), Partial(sum)), tensor_meta=TensorMeta(shape=torch.Size([1024, 2048]), stride=(2048, 1), dtype=torch.float32)), redistribute_schema=None, needs_redistribute=False)

Output after matmul: (Shard(dim=1), Partial(sum)) on shape torch.Size([1024, 2048])

Output placement (Shard(dim=1), Partial(sum)) doesn't match input [Shard(dim=0), Shard(dim=1)]
Redistributing...
Output has Partial placement - will be reduced during redistribution
[rank2]:V0615 22:46:40.556000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank4]:V0615 22:46:40.556000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank6]:V0615 22:46:40.556000 19632 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank0]:V0615 22:46:40.556000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from P to R on mesh dim 1
[rank2]:V0615 22:46:40.556000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank4]:V0615 22:46:40.557000 19630 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank0]:V0615 22:46:40.557000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from S(1) to S(0) on mesh dim 0
[rank2]:V0615 22:46:40.557000 19628 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
[rank0]:V0615 22:46:40.557000 19626 .venv/lib/python3.12/site-packages/torch/distributed/tensor/_redistribute.py:189] redistribute from R to S(1) on mesh dim 1
Final output: (Shard(dim=0), Shard(dim=1)) on shape torch.Size([1024, 2048])

============================================================
COMMUNICATION SUMMARY:
============================================================
c10d_functional.all_gather_into_tensor: 1
_dtensor.shard_dim_alltoall: 2
c10d_functional.all_reduce: 2

WARNING: All-reduce was used! This doesn't match the JAX pattern.
